{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kTKaVK47knP"
      },
      "source": [
        "# Neural Language Model for \"Sing A Song of Sixpence\" & \"Stopping by Woods on a Snowy Evening\"\n",
        "\n",
        "In this project, I attempt to create a RNN for predicting the next few characters for lines in two popular poems, applying LSTM to a language prediction problem.\n",
        "\n",
        "A language model predicts the next word in the sequence based on the specific words that have come before it in the sequence.\n",
        "\n",
        "It is also possible to develop language models at the character level using neural networks. The benefit of character-based language models is their small vocabulary and flexibility in handling any words, punctuation, and other document structure. This comes at the cost of requiring larger models that are slower to train.\n",
        "\n",
        "Nevertheless, in the field of neural language models, character-based models offer a lot of promise for a general, flexible and powerful approach to language modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIs5eFI97knR"
      },
      "source": [
        "# Source Text Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wl1ZLGG7wii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e412ea90-5641-4a66-c346-afe441455034"
      },
      "source": [
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install h5py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.2)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (3.9.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from h5py) (1.23.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqQdG7M68HVt"
      },
      "source": [
        "\n",
        "s='Sing a song of sixpence,\\\n",
        "A pocket full of rye.\\\n",
        "Four and twenty blackbirds,\\\n",
        "Baked in a pie.\\\n",
        "When the pie was opened\\\n",
        "The birds began to sing;\\\n",
        "Wasnâ€™t that a dainty dish,\\\n",
        "To set before the king.\\\n",
        "The king was in his counting house,\\\n",
        "Counting out his money;\\\n",
        "The queen was in the parlour,\\\n",
        "Eating bread and honey.\\\n",
        "The maid was in the garden,\\\n",
        "Hanging out the clothes,\\\n",
        "When down came a blackbird\\\n",
        "And pecked off her nose.'\n",
        "\n",
        "with open('rhymes.txt','w') as f:\n",
        "  f.write(s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq7wq3fF7knV"
      },
      "source": [
        "    Sing a song of sixpence,\n",
        "    A pocket full of rye.\n",
        "    Four and twenty blackbirds,\n",
        "    Baked in a pie.\n",
        "\n",
        "    When the pie was opened\n",
        "    The birds began to sing;\n",
        "    Wasnâ€™t that a dainty dish,\n",
        "    To set before the king.\n",
        "\n",
        "    The king was in his counting house,\n",
        "    Counting out his money;\n",
        "    The queen was in the parlour,\n",
        "    Eating bread and honey.\n",
        "\n",
        "    The maid was in the garden,\n",
        "    Hanging out the clothes,\n",
        "    When down came a blackbird\n",
        "    And pecked off her nose."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdOJmdjU7knW"
      },
      "source": [
        "# Sequence Generation\n",
        "\n",
        "A language model must be trained on the text, and in the case of a character-based language model, the input and output sequences must be characters.\n",
        "\n",
        "The number of characters used as input will also define the number of characters that will need to be provided to the model in order to elicit the first predicted character.\n",
        "\n",
        "After the first character has been generated, it can be appended to the input sequence and used as input for the model to generate the next character.\n",
        "\n",
        "Longer sequences offer more context for the model to learn what character to output next but take longer to train and impose more burden on seeding the model when generating text.\n",
        "\n",
        "We will use an arbitrary length of 10 characters for this model.\n",
        "\n",
        "There is not a lot of text, and 10 characters is a few words.\n",
        "\n",
        "We can now transform the raw text into a form that our model can learn; specifically, input and output sequences of characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjJfvv1U7knY"
      },
      "source": [
        "#load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text\n",
        "\n",
        "# save tokens to file, one dialog per line\n",
        "def save_doc(lines, filename):\n",
        "    data = '\\n'.join(lines)\n",
        "    file = open(filename, 'w')\n",
        "    file.write(data)\n",
        "    file.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6GbMmMs7knb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64f599eb-5e38-4912-a409-335857a43c62"
      },
      "source": [
        "#load text\n",
        "raw_text = load_doc('rhymes.txt')\n",
        "print(raw_text)\n",
        "\n",
        "# clean\n",
        "tokens = raw_text.split()\n",
        "raw_text = ' '.join(tokens)\n",
        "\n",
        "# organize into sequences of characters\n",
        "length = 10\n",
        "sequences = list()\n",
        "for i in range(length, len(raw_text)):\n",
        "    # select sequence of tokens\n",
        "    seq = raw_text[i-length:i+1]\n",
        "    # store\n",
        "    sequences.append(seq)\n",
        "print('Total Sequences: %d' % len(sequences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sing a song of sixpence,A pocket full of rye.Four and twenty blackbirds,Baked in a pie.When the pie was openedThe birds began to sing;Wasnâ€™t that a dainty dish,To set before the king.The king was in his counting house,Counting out his money;The queen was in the parlour,Eating bread and honey.The maid was in the garden,Hanging out the clothes,When down came a blackbirdAnd pecked off her nose.\n",
            "Total Sequences: 384\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBepEyDn7kne"
      },
      "source": [
        "# save sequences to file\n",
        "out_filename = 'char_sequences.txt'\n",
        "save_doc(sequences, out_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohjVbv1l7kng"
      },
      "source": [
        "# Model Training\n",
        "\n",
        "The model will read encoded characters and predict the next character in the sequence. A Long Short-Term Memory recurrent neural network hidden layer will be used to learn the context from the input sequence in order to make the predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HBgtQvY7knh"
      },
      "source": [
        "from numpy import array\n",
        "from pickle import dump\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import LSTM\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "    # open the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    # close the file\n",
        "    file.close()\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8ruXoPS7knl"
      },
      "source": [
        "# load\n",
        "\n",
        "in_filename = 'char_sequences.txt'\n",
        "raw_text = load_doc(in_filename)\n",
        "lines = raw_text.split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTnvoY-f7kno"
      },
      "source": [
        "The sequences of characters must be encoded as integers.This means that each unique character will be assigned a specific integer value and each sequence of characters will be encoded as a sequence of integers. We can create the mapping given a sorted set of unique characters in the raw input data. The mapping is a dictionary of character values to integer values.\n",
        "\n",
        "Next, we can process each sequence of characters one at a time and use the dictionary mapping to look up the integer value for each character. The result is a list of integer lists.\n",
        "\n",
        "We need to know the size of the vocabulary later. We can retrieve this as the size of the dictionary mapping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMndzt5v7kno",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1de524c6-00c4-4a7f-e479-7c60ed8c6661"
      },
      "source": [
        "# integer encode sequences of characters\n",
        "chars = sorted(list(set(raw_text)))\n",
        "mapping = dict((c, i) for i, c in enumerate(chars))\n",
        "sequences = list()\n",
        "for line in lines:\n",
        "    # integer encode line\n",
        "    encoded_seq = [mapping[char] for char in line]\n",
        "    # store\n",
        "    sequences.append(encoded_seq)\n",
        "\n",
        "# vocabulary size\n",
        "vocab_size = len(mapping)\n",
        "print('Vocabulary Size: %d' % vocab_size)\n",
        "\n",
        "# separate into input and output\n",
        "sequences = array(sequences)\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
        "X = array(sequences)\n",
        "y = to_categorical(y, num_classes=vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qyCICfV7knx"
      },
      "source": [
        "The model is defined with an input layer that takes sequences that have 10 time steps and 38 features for the one hot encoded input sequences. Rather than specify these numbers, we use the second and third dimensions on the X input data. This is so that if we change the length of the sequences or size of the vocabulary, we do not need to change the model definition.\n",
        "\n",
        "The model has a single LSTM hidden layer with 75 memory cells. The model has a fully connected output layer that outputs one vector with a probability distribution across all characters in the vocabulary. A softmax activation function is used on the output layer to ensure the output has the properties of a probability distribution.\n",
        "\n",
        "The model is learning a multi-class classification problem, therefore we use the categorical log loss intended for this type of problem. The efficient Adam implementation of gradient descent is used to optimize the model and accuracy is reported at the end of each batch update. The model is fit for 50 training epochs."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----- ORIGINAL ------\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())\n",
        "# compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "history=model.fit(X, y, epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMQYjmsH8oIC",
        "outputId": "31dde15e-19a7-4bbc-a28d-a39f2284e4b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 75)                34200     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 38)                2888      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 37088 (144.88 KB)\n",
            "Trainable params: 37088 (144.88 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/100\n",
            "12/12 [==============================] - 3s 10ms/step - loss: 3.6134 - accuracy: 0.0651\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 3.5368 - accuracy: 0.1589\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 3.3178 - accuracy: 0.1589\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 3.1184 - accuracy: 0.1589\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 3.0676 - accuracy: 0.1641\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 3.0486 - accuracy: 0.1589\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 3.0227 - accuracy: 0.1615\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 3.0090 - accuracy: 0.1615\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2.9926 - accuracy: 0.1719\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2.9737 - accuracy: 0.1667\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2.9507 - accuracy: 0.1771\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2.9170 - accuracy: 0.1979\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 2.8936 - accuracy: 0.1901\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 2.8676 - accuracy: 0.2005\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 2.8355 - accuracy: 0.2031\n",
            "Epoch 16/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 2.8028 - accuracy: 0.2135\n",
            "Epoch 17/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 2.7790 - accuracy: 0.2161\n",
            "Epoch 18/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 2.7551 - accuracy: 0.2057\n",
            "Epoch 19/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 2.7054 - accuracy: 0.2109\n",
            "Epoch 20/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 2.6788 - accuracy: 0.2526\n",
            "Epoch 21/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 2.6399 - accuracy: 0.2578\n",
            "Epoch 22/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 2.5983 - accuracy: 0.2448\n",
            "Epoch 23/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 2.5465 - accuracy: 0.2812\n",
            "Epoch 24/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 2.5120 - accuracy: 0.2917\n",
            "Epoch 25/100\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 2.4899 - accuracy: 0.2943\n",
            "Epoch 26/100\n",
            "12/12 [==============================] - 0s 28ms/step - loss: 2.4548 - accuracy: 0.3151\n",
            "Epoch 27/100\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 2.4007 - accuracy: 0.3281\n",
            "Epoch 28/100\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 2.3791 - accuracy: 0.3307\n",
            "Epoch 29/100\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 2.3101 - accuracy: 0.3307\n",
            "Epoch 30/100\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 2.2661 - accuracy: 0.3646\n",
            "Epoch 31/100\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 2.2410 - accuracy: 0.3542\n",
            "Epoch 32/100\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 2.2054 - accuracy: 0.3776\n",
            "Epoch 33/100\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 2.1659 - accuracy: 0.3490\n",
            "Epoch 34/100\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 2.1193 - accuracy: 0.4036\n",
            "Epoch 35/100\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 2.0756 - accuracy: 0.3828\n",
            "Epoch 36/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 2.0387 - accuracy: 0.4271\n",
            "Epoch 37/100\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 2.0005 - accuracy: 0.4271\n",
            "Epoch 38/100\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 1.9439 - accuracy: 0.4453\n",
            "Epoch 39/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.8962 - accuracy: 0.4609\n",
            "Epoch 40/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.8554 - accuracy: 0.4661\n",
            "Epoch 41/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.8284 - accuracy: 0.4792\n",
            "Epoch 42/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.7769 - accuracy: 0.5000\n",
            "Epoch 43/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 1.7415 - accuracy: 0.5104\n",
            "Epoch 44/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 1.7017 - accuracy: 0.5391\n",
            "Epoch 45/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 1.6616 - accuracy: 0.5130\n",
            "Epoch 46/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 1.6221 - accuracy: 0.5469\n",
            "Epoch 47/100\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 1.5785 - accuracy: 0.5859\n",
            "Epoch 48/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 1.5434 - accuracy: 0.5938\n",
            "Epoch 49/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 1.4921 - accuracy: 0.6302\n",
            "Epoch 50/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 1.4487 - accuracy: 0.6198\n",
            "Epoch 51/100\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 1.4026 - accuracy: 0.6458\n",
            "Epoch 52/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.3639 - accuracy: 0.6771\n",
            "Epoch 53/100\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 1.3504 - accuracy: 0.6510\n",
            "Epoch 54/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.2975 - accuracy: 0.6693\n",
            "Epoch 55/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 1.2492 - accuracy: 0.6953\n",
            "Epoch 56/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 1.2201 - accuracy: 0.7161\n",
            "Epoch 57/100\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 1.1687 - accuracy: 0.7500\n",
            "Epoch 58/100\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 1.1408 - accuracy: 0.7500\n",
            "Epoch 59/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 1.1110 - accuracy: 0.7500\n",
            "Epoch 60/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 1.0871 - accuracy: 0.7630\n",
            "Epoch 61/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 1.0466 - accuracy: 0.7760\n",
            "Epoch 62/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 1.0055 - accuracy: 0.7943\n",
            "Epoch 63/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.9751 - accuracy: 0.8203\n",
            "Epoch 64/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.9550 - accuracy: 0.8151\n",
            "Epoch 65/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.9058 - accuracy: 0.8255\n",
            "Epoch 66/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.8797 - accuracy: 0.8542\n",
            "Epoch 67/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.8529 - accuracy: 0.8542\n",
            "Epoch 68/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.8350 - accuracy: 0.8568\n",
            "Epoch 69/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.8023 - accuracy: 0.8620\n",
            "Epoch 70/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.7627 - accuracy: 0.8932\n",
            "Epoch 71/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.7562 - accuracy: 0.8880\n",
            "Epoch 72/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.7372 - accuracy: 0.8906\n",
            "Epoch 73/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.6959 - accuracy: 0.9115\n",
            "Epoch 74/100\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.6629 - accuracy: 0.9245\n",
            "Epoch 75/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.6328 - accuracy: 0.9271\n",
            "Epoch 76/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.6262 - accuracy: 0.9245\n",
            "Epoch 77/100\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.6048 - accuracy: 0.9271\n",
            "Epoch 78/100\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.5802 - accuracy: 0.9401\n",
            "Epoch 79/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.5500 - accuracy: 0.9453\n",
            "Epoch 80/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.5290 - accuracy: 0.9505\n",
            "Epoch 81/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.5160 - accuracy: 0.9479\n",
            "Epoch 82/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.5033 - accuracy: 0.9557\n",
            "Epoch 83/100\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.4832 - accuracy: 0.9609\n",
            "Epoch 84/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 0.4640 - accuracy: 0.9688\n",
            "Epoch 85/100\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.4415 - accuracy: 0.9661\n",
            "Epoch 86/100\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.4197 - accuracy: 0.9661\n",
            "Epoch 87/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.4140 - accuracy: 0.9688\n",
            "Epoch 88/100\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.4079 - accuracy: 0.9609\n",
            "Epoch 89/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.3829 - accuracy: 0.9792\n",
            "Epoch 90/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 0.3671 - accuracy: 0.9740\n",
            "Epoch 91/100\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.3528 - accuracy: 0.9792\n",
            "Epoch 92/100\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.3367 - accuracy: 0.9818\n",
            "Epoch 93/100\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.3315 - accuracy: 0.9818\n",
            "Epoch 94/100\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 0.3219 - accuracy: 0.9844\n",
            "Epoch 95/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.3115 - accuracy: 0.9818\n",
            "Epoch 96/100\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.3030 - accuracy: 0.9896\n",
            "Epoch 97/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.2885 - accuracy: 0.9818\n",
            "Epoch 98/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.2776 - accuracy: 0.9896\n",
            "Epoch 99/100\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.2618 - accuracy: 0.9922\n",
            "Epoch 100/100\n",
            "12/12 [==============================] - 0s 26ms/step - loss: 0.2545 - accuracy: 0.9896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raAPR9Qp7kn1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc777b75-2cbb-43b5-c913-998e2402fbdb"
      },
      "source": [
        "# save the model to file\n",
        "model.save('model.h5')\n",
        "# save the mapping\n",
        "dump(mapping, open('mapping.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Improving the Model**"
      ],
      "metadata": {
        "id": "qIEOn6t2IHtN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "K1jO9lYX7kny",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e51d0045-7f6a-40b3-e410-099b99ee4b5c"
      },
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "# define model\n",
        "model_2 = Sequential()\n",
        "\n",
        "model_2.add(LSTM(100, input_shape=(X.shape[1], X.shape[2])))\n",
        "model_2.add(Dropout(0.2))\n",
        "\n",
        "model_2.add(Dense(80, activation='relu'))\n",
        "model_2.add(Dropout(0.2))\n",
        "model_2.add(Dense(75, activation='relu'))\n",
        "model_2.add(Dropout(0.2))\n",
        "model_2.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "print(model_2.summary())\n",
        "# compile model\n",
        "model_2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "history=model_2.fit(X, y, epochs=120)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_34\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_41 (LSTM)              (None, 100)               55600     \n",
            "                                                                 \n",
            " dropout_28 (Dropout)        (None, 100)               0         \n",
            "                                                                 \n",
            " dense_48 (Dense)            (None, 80)                8080      \n",
            "                                                                 \n",
            " dropout_29 (Dropout)        (None, 80)                0         \n",
            "                                                                 \n",
            " dense_49 (Dense)            (None, 75)                6075      \n",
            "                                                                 \n",
            " dropout_30 (Dropout)        (None, 75)                0         \n",
            "                                                                 \n",
            " dense_50 (Dense)            (None, 38)                2888      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 72643 (283.76 KB)\n",
            "Trainable params: 72643 (283.76 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/120\n",
            "12/12 [==============================] - 2s 9ms/step - loss: 3.6159 - accuracy: 0.0286\n",
            "Epoch 2/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 3.4935 - accuracy: 0.0625\n",
            "Epoch 3/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 3.2262 - accuracy: 0.1693\n",
            "Epoch 4/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 3.1867 - accuracy: 0.1562\n",
            "Epoch 5/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 3.1455 - accuracy: 0.1589\n",
            "Epoch 6/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 3.1300 - accuracy: 0.1562\n",
            "Epoch 7/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 3.0868 - accuracy: 0.1458\n",
            "Epoch 8/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 3.0543 - accuracy: 0.1615\n",
            "Epoch 9/120\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 3.0607 - accuracy: 0.1641\n",
            "Epoch 10/120\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 3.0543 - accuracy: 0.1641\n",
            "Epoch 11/120\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 3.0530 - accuracy: 0.1484\n",
            "Epoch 12/120\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 3.0277 - accuracy: 0.1510\n",
            "Epoch 13/120\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 2.9783 - accuracy: 0.1823\n",
            "Epoch 14/120\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 2.9564 - accuracy: 0.1771\n",
            "Epoch 15/120\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 2.9199 - accuracy: 0.1562\n",
            "Epoch 16/120\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 2.8819 - accuracy: 0.1953\n",
            "Epoch 17/120\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 2.8385 - accuracy: 0.1849\n",
            "Epoch 18/120\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 2.8364 - accuracy: 0.2031\n",
            "Epoch 19/120\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 2.7868 - accuracy: 0.1927\n",
            "Epoch 20/120\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 2.7225 - accuracy: 0.2135\n",
            "Epoch 21/120\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 2.6903 - accuracy: 0.2396\n",
            "Epoch 22/120\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 2.6827 - accuracy: 0.2318\n",
            "Epoch 23/120\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 2.5859 - accuracy: 0.2448\n",
            "Epoch 24/120\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 2.5259 - accuracy: 0.2682\n",
            "Epoch 25/120\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 2.4785 - accuracy: 0.2656\n",
            "Epoch 26/120\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 2.4358 - accuracy: 0.2839\n",
            "Epoch 27/120\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 2.3963 - accuracy: 0.3177\n",
            "Epoch 28/120\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 2.3064 - accuracy: 0.3047\n",
            "Epoch 29/120\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 2.2827 - accuracy: 0.3490\n",
            "Epoch 30/120\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 2.2789 - accuracy: 0.2839\n",
            "Epoch 31/120\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 2.2255 - accuracy: 0.3333\n",
            "Epoch 32/120\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 2.1172 - accuracy: 0.3932\n",
            "Epoch 33/120\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 2.1136 - accuracy: 0.3750\n",
            "Epoch 34/120\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 2.1418 - accuracy: 0.3438\n",
            "Epoch 35/120\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 2.0489 - accuracy: 0.3932\n",
            "Epoch 36/120\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 1.9481 - accuracy: 0.4193\n",
            "Epoch 37/120\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 1.9732 - accuracy: 0.4245\n",
            "Epoch 38/120\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1.9070 - accuracy: 0.4401\n",
            "Epoch 39/120\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1.8418 - accuracy: 0.4036\n",
            "Epoch 40/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.8319 - accuracy: 0.4427\n",
            "Epoch 41/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.7440 - accuracy: 0.4635\n",
            "Epoch 42/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.7867 - accuracy: 0.4766\n",
            "Epoch 43/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.6850 - accuracy: 0.5026\n",
            "Epoch 44/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.7258 - accuracy: 0.4766\n",
            "Epoch 45/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.6206 - accuracy: 0.5000\n",
            "Epoch 46/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.6011 - accuracy: 0.4922\n",
            "Epoch 47/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.6237 - accuracy: 0.4922\n",
            "Epoch 48/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.5676 - accuracy: 0.5078\n",
            "Epoch 49/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.5230 - accuracy: 0.5130\n",
            "Epoch 50/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.4662 - accuracy: 0.5312\n",
            "Epoch 51/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.4128 - accuracy: 0.5417\n",
            "Epoch 52/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.4530 - accuracy: 0.5312\n",
            "Epoch 53/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.3923 - accuracy: 0.5547\n",
            "Epoch 54/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.4025 - accuracy: 0.5443\n",
            "Epoch 55/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.3510 - accuracy: 0.5729\n",
            "Epoch 56/120\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 1.2865 - accuracy: 0.5833\n",
            "Epoch 57/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.2683 - accuracy: 0.5807\n",
            "Epoch 58/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.2496 - accuracy: 0.6042\n",
            "Epoch 59/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.2191 - accuracy: 0.5781\n",
            "Epoch 60/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.2446 - accuracy: 0.5833\n",
            "Epoch 61/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1537 - accuracy: 0.6094\n",
            "Epoch 62/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1355 - accuracy: 0.6276\n",
            "Epoch 63/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.1407 - accuracy: 0.6432\n",
            "Epoch 64/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.0764 - accuracy: 0.6302\n",
            "Epoch 65/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.1010 - accuracy: 0.6510\n",
            "Epoch 66/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.0327 - accuracy: 0.6589\n",
            "Epoch 67/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.9819 - accuracy: 0.6667\n",
            "Epoch 68/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.9899 - accuracy: 0.6745\n",
            "Epoch 69/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.9670 - accuracy: 0.6901\n",
            "Epoch 70/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.9218 - accuracy: 0.6823\n",
            "Epoch 71/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.9133 - accuracy: 0.7188\n",
            "Epoch 72/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.8960 - accuracy: 0.6771\n",
            "Epoch 73/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.9236 - accuracy: 0.7135\n",
            "Epoch 74/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.8694 - accuracy: 0.7214\n",
            "Epoch 75/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.8780 - accuracy: 0.7005\n",
            "Epoch 76/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.8270 - accuracy: 0.7318\n",
            "Epoch 77/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.8193 - accuracy: 0.7266\n",
            "Epoch 78/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.7594 - accuracy: 0.7474\n",
            "Epoch 79/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.7465 - accuracy: 0.7604\n",
            "Epoch 80/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.7596 - accuracy: 0.7526\n",
            "Epoch 81/120\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.7436 - accuracy: 0.7708\n",
            "Epoch 82/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.7285 - accuracy: 0.7578\n",
            "Epoch 83/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6722 - accuracy: 0.7630\n",
            "Epoch 84/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.7155 - accuracy: 0.7578\n",
            "Epoch 85/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.6731 - accuracy: 0.7786\n",
            "Epoch 86/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6225 - accuracy: 0.7839\n",
            "Epoch 87/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.5985 - accuracy: 0.8255\n",
            "Epoch 88/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.6791 - accuracy: 0.7656\n",
            "Epoch 89/120\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.6229 - accuracy: 0.7969\n",
            "Epoch 90/120\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.6470 - accuracy: 0.7708\n",
            "Epoch 91/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.5274 - accuracy: 0.8307\n",
            "Epoch 92/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.6054 - accuracy: 0.8021\n",
            "Epoch 93/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.5788 - accuracy: 0.8203\n",
            "Epoch 94/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.5126 - accuracy: 0.8151\n",
            "Epoch 95/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.5659 - accuracy: 0.8047\n",
            "Epoch 96/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.5556 - accuracy: 0.8151\n",
            "Epoch 97/120\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.5266 - accuracy: 0.8203\n",
            "Epoch 98/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.4952 - accuracy: 0.8490\n",
            "Epoch 99/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.4889 - accuracy: 0.8151\n",
            "Epoch 100/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.4962 - accuracy: 0.8438\n",
            "Epoch 101/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.4981 - accuracy: 0.8438\n",
            "Epoch 102/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4780 - accuracy: 0.8594\n",
            "Epoch 103/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.4186 - accuracy: 0.8594\n",
            "Epoch 104/120\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4355 - accuracy: 0.8672\n",
            "Epoch 105/120\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.4291 - accuracy: 0.8646\n",
            "Epoch 106/120\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.4391 - accuracy: 0.8490\n",
            "Epoch 107/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.4216 - accuracy: 0.8490\n",
            "Epoch 108/120\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.4463 - accuracy: 0.8802\n",
            "Epoch 109/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.3930 - accuracy: 0.8672\n",
            "Epoch 110/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.4136 - accuracy: 0.8542\n",
            "Epoch 111/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.4180 - accuracy: 0.8698\n",
            "Epoch 112/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.3780 - accuracy: 0.8854\n",
            "Epoch 113/120\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.3565 - accuracy: 0.8880\n",
            "Epoch 114/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.3681 - accuracy: 0.8880\n",
            "Epoch 115/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.3306 - accuracy: 0.9036\n",
            "Epoch 116/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.3543 - accuracy: 0.8932\n",
            "Epoch 117/120\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.3032 - accuracy: 0.9089\n",
            "Epoch 118/120\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.3351 - accuracy: 0.8776\n",
            "Epoch 119/120\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.2781 - accuracy: 0.8932\n",
            "Epoch 120/120\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.3886 - accuracy: 0.8724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Process for RNN**"
      ],
      "metadata": {
        "id": "qQ0vJ9egIOTd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When formulating my own RNN, I tried a variety of different techniques to get the best possible output while mitigating for overfitting:\n",
        "\n",
        "1) Using different number of memory cells\n",
        "\n",
        "Memory cells in the context of RNNs are roughly the \"# of neurons\" in the model. Therefore, intuitively, I believed it would be better to increase the number of memory cells because there would be more nodes that would account for some of the complexities within the data. However, I did not want to massively increase the number of memory cells as that would overfit. So, I increased the number of memory cells by a small amount to be at 100 versus the previous number of 75. When I ran the model with just this change, the training accuracy was slightly higher while the final model did not change in accuracy either.\n",
        "\n",
        "2) Different types and numbers of layers\n",
        "\n",
        "I knew that I wanted to add more layers to the model because it would again account for some of the hidden complexities within the model. Yet, because I already chose to increase the number of nodes, I thought to add only 1 more layer. I specifically chose to add a fully connected ReLU layer because I know that this kind of activation layer specifically accounts for any behavior that's not linear and is typically pretty efficient as it doesn't change any kind of performance among neurons that are doing a good job but only alters the nodes that do need further manipulation. I did at first try to include multiple ReLU layers but that ultimately produced a very inaccurate model output in the testing portion.\n",
        "\n",
        "3) Different lengths of training epochs\n",
        "\n",
        "I added a few more epochs because I believed that I was accounting for some overfitting already and wanted to ensure that the model was still getting exposure to the dataset that it needed. I settled on the epoch number of 120 because it didn't go too significantly over the starting number of 100.\n",
        "\n",
        "4) Different sequence lengths and pre-processing\n",
        "\n",
        "I changed the sequence length to be 24, the length of the first line, at first. However, it ended up performing very well on my training model and validation epochs but very poorly when we tried to generate text, showing that it had overfitted. I therefore kept the sequence length at 10 so that my model could be trained on smaller bits of the corpus and be able to understand patterns in that manner.\n",
        "\n",
        "5) Try regularization techniques such as Dropout\n",
        "\n",
        "Because I changed all other aspects of the model to be more specific to the corpus, I made sure to include 3 layers of Dropout with small parameters in order to regularize the model and generalize it for the future."
      ],
      "metadata": {
        "id": "oO94tD95IRYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Evaluation**"
      ],
      "metadata": {
        "id": "23pifMkgR4UX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def validation_epochs(X, y):\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "  model_2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  history = model_2.fit(\n",
        "      X_train, y_train,\n",
        "      epochs=100,\n",
        "      validation_data=(X_val, y_val),\n",
        "      verbose=1)\n",
        "  return history"
      ],
      "metadata": {
        "id": "UgaQP5VAE2ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_epochs(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jDiY5KdGral",
        "outputId": "1ea03bdb-31e8-44c6-f01d-9d67d669ac87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "10/10 [==============================] - 4s 85ms/step - loss: 0.3692 - accuracy: 0.8795 - val_loss: 0.0989 - val_accuracy: 0.9870\n",
            "Epoch 2/100\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.3419 - accuracy: 0.8827 - val_loss: 0.1538 - val_accuracy: 0.9351\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - 0s 16ms/step - loss: 0.3452 - accuracy: 0.8893 - val_loss: 0.1814 - val_accuracy: 0.9351\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - 0s 16ms/step - loss: 0.2947 - accuracy: 0.9121 - val_loss: 0.2027 - val_accuracy: 0.9481\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - 0s 16ms/step - loss: 0.2596 - accuracy: 0.9479 - val_loss: 0.1922 - val_accuracy: 0.9610\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - 0s 16ms/step - loss: 0.3132 - accuracy: 0.8893 - val_loss: 0.1792 - val_accuracy: 0.9740\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.2746 - accuracy: 0.9218 - val_loss: 0.2227 - val_accuracy: 0.9481\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - 0s 25ms/step - loss: 0.2627 - accuracy: 0.9055 - val_loss: 0.1777 - val_accuracy: 0.9610\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - 0s 30ms/step - loss: 0.2410 - accuracy: 0.9316 - val_loss: 0.1892 - val_accuracy: 0.9481\n",
            "Epoch 10/100\n",
            "10/10 [==============================] - 0s 26ms/step - loss: 0.2389 - accuracy: 0.9283 - val_loss: 0.1647 - val_accuracy: 0.9610\n",
            "Epoch 11/100\n",
            "10/10 [==============================] - 0s 30ms/step - loss: 0.2476 - accuracy: 0.9088 - val_loss: 0.1684 - val_accuracy: 0.9610\n",
            "Epoch 12/100\n",
            "10/10 [==============================] - 0s 25ms/step - loss: 0.2754 - accuracy: 0.9055 - val_loss: 0.1960 - val_accuracy: 0.9610\n",
            "Epoch 13/100\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.2014 - accuracy: 0.9349 - val_loss: 0.2133 - val_accuracy: 0.9481\n",
            "Epoch 14/100\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.2308 - accuracy: 0.9121 - val_loss: 0.1729 - val_accuracy: 0.9740\n",
            "Epoch 15/100\n",
            "10/10 [==============================] - 0s 29ms/step - loss: 0.2200 - accuracy: 0.9349 - val_loss: 0.2127 - val_accuracy: 0.9351\n",
            "Epoch 16/100\n",
            "10/10 [==============================] - 0s 29ms/step - loss: 0.2348 - accuracy: 0.9283 - val_loss: 0.2676 - val_accuracy: 0.9091\n",
            "Epoch 17/100\n",
            "10/10 [==============================] - 0s 29ms/step - loss: 0.2417 - accuracy: 0.9251 - val_loss: 0.3015 - val_accuracy: 0.9221\n",
            "Epoch 18/100\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.2970 - accuracy: 0.9088 - val_loss: 0.3096 - val_accuracy: 0.8831\n",
            "Epoch 19/100\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.2218 - accuracy: 0.9349 - val_loss: 0.2821 - val_accuracy: 0.8961\n",
            "Epoch 20/100\n",
            "10/10 [==============================] - 0s 30ms/step - loss: 0.2354 - accuracy: 0.9186 - val_loss: 0.3386 - val_accuracy: 0.8961\n",
            "Epoch 21/100\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.2066 - accuracy: 0.9446 - val_loss: 0.3342 - val_accuracy: 0.8831\n",
            "Epoch 22/100\n",
            "10/10 [==============================] - 0s 28ms/step - loss: 0.1915 - accuracy: 0.9381 - val_loss: 0.2776 - val_accuracy: 0.9221\n",
            "Epoch 23/100\n",
            "10/10 [==============================] - 0s 29ms/step - loss: 0.2385 - accuracy: 0.9218 - val_loss: 0.2782 - val_accuracy: 0.8961\n",
            "Epoch 24/100\n",
            "10/10 [==============================] - 0s 17ms/step - loss: 0.2120 - accuracy: 0.9381 - val_loss: 0.4687 - val_accuracy: 0.8701\n",
            "Epoch 25/100\n",
            "10/10 [==============================] - 0s 16ms/step - loss: 0.2183 - accuracy: 0.9414 - val_loss: 0.3537 - val_accuracy: 0.8701\n",
            "Epoch 26/100\n",
            "10/10 [==============================] - 0s 17ms/step - loss: 0.2248 - accuracy: 0.9251 - val_loss: 0.4480 - val_accuracy: 0.8701\n",
            "Epoch 27/100\n",
            "10/10 [==============================] - 0s 17ms/step - loss: 0.2185 - accuracy: 0.9283 - val_loss: 0.5081 - val_accuracy: 0.8571\n",
            "Epoch 28/100\n",
            "10/10 [==============================] - 0s 17ms/step - loss: 0.1766 - accuracy: 0.9479 - val_loss: 0.5066 - val_accuracy: 0.8701\n",
            "Epoch 29/100\n",
            "10/10 [==============================] - 0s 16ms/step - loss: 0.2383 - accuracy: 0.9251 - val_loss: 0.3716 - val_accuracy: 0.8831\n",
            "Epoch 30/100\n",
            "10/10 [==============================] - 0s 17ms/step - loss: 0.2051 - accuracy: 0.9153 - val_loss: 0.4149 - val_accuracy: 0.8701\n",
            "Epoch 31/100\n",
            "10/10 [==============================] - 0s 18ms/step - loss: 0.1772 - accuracy: 0.9446 - val_loss: 0.4071 - val_accuracy: 0.8961\n",
            "Epoch 32/100\n",
            "10/10 [==============================] - 0s 17ms/step - loss: 0.1907 - accuracy: 0.9414 - val_loss: 0.4237 - val_accuracy: 0.8571\n",
            "Epoch 33/100\n",
            "10/10 [==============================] - 0s 18ms/step - loss: 0.1986 - accuracy: 0.9381 - val_loss: 0.3928 - val_accuracy: 0.8831\n",
            "Epoch 34/100\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.1940 - accuracy: 0.9349 - val_loss: 0.3247 - val_accuracy: 0.9091\n",
            "Epoch 35/100\n",
            "10/10 [==============================] - 0s 17ms/step - loss: 0.1605 - accuracy: 0.9511 - val_loss: 0.3604 - val_accuracy: 0.9091\n",
            "Epoch 36/100\n",
            "10/10 [==============================] - 0s 17ms/step - loss: 0.1695 - accuracy: 0.9414 - val_loss: 0.4074 - val_accuracy: 0.8442\n",
            "Epoch 37/100\n",
            "10/10 [==============================] - 0s 16ms/step - loss: 0.1844 - accuracy: 0.9414 - val_loss: 0.4045 - val_accuracy: 0.8961\n",
            "Epoch 38/100\n",
            "10/10 [==============================] - 0s 16ms/step - loss: 0.1771 - accuracy: 0.9479 - val_loss: 0.4392 - val_accuracy: 0.8571\n",
            "Epoch 39/100\n",
            "10/10 [==============================] - 0s 17ms/step - loss: 0.1497 - accuracy: 0.9446 - val_loss: 0.5560 - val_accuracy: 0.8182\n",
            "Epoch 40/100\n",
            "10/10 [==============================] - 0s 17ms/step - loss: 0.1963 - accuracy: 0.9218 - val_loss: 0.4571 - val_accuracy: 0.8442\n",
            "Epoch 41/100\n",
            "10/10 [==============================] - 0s 17ms/step - loss: 0.2288 - accuracy: 0.9316 - val_loss: 0.4556 - val_accuracy: 0.8312\n",
            "Epoch 42/100\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.1030 - accuracy: 0.9805 - val_loss: 0.4475 - val_accuracy: 0.8442\n",
            "Epoch 43/100\n",
            "10/10 [==============================] - 0s 18ms/step - loss: 0.1711 - accuracy: 0.9414 - val_loss: 0.4289 - val_accuracy: 0.8831\n",
            "Epoch 44/100\n",
            "10/10 [==============================] - 0s 17ms/step - loss: 0.2099 - accuracy: 0.9381 - val_loss: 0.4769 - val_accuracy: 0.8312\n",
            "Epoch 45/100\n",
            "10/10 [==============================] - 0s 17ms/step - loss: 0.1711 - accuracy: 0.9609 - val_loss: 0.5612 - val_accuracy: 0.8442\n",
            "Epoch 46/100\n",
            "10/10 [==============================] - 0s 17ms/step - loss: 0.1754 - accuracy: 0.9381 - val_loss: 0.3560 - val_accuracy: 0.8961\n",
            "Epoch 47/100\n",
            "10/10 [==============================] - 0s 16ms/step - loss: 0.1523 - accuracy: 0.9446 - val_loss: 0.4500 - val_accuracy: 0.8571\n",
            "Epoch 48/100\n",
            "10/10 [==============================] - 0s 17ms/step - loss: 0.1472 - accuracy: 0.9707 - val_loss: 0.5941 - val_accuracy: 0.8571\n",
            "Epoch 49/100\n",
            "10/10 [==============================] - 0s 16ms/step - loss: 0.1151 - accuracy: 0.9707 - val_loss: 0.5521 - val_accuracy: 0.8442\n",
            "Epoch 50/100\n",
            "10/10 [==============================] - 0s 17ms/step - loss: 0.1348 - accuracy: 0.9511 - val_loss: 0.5282 - val_accuracy: 0.8571\n",
            "Epoch 51/100\n",
            "10/10 [==============================] - 0s 18ms/step - loss: 0.1643 - accuracy: 0.9414 - val_loss: 0.5333 - val_accuracy: 0.8701\n",
            "Epoch 52/100\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.1747 - accuracy: 0.9414 - val_loss: 0.5479 - val_accuracy: 0.8182\n",
            "Epoch 53/100\n",
            "10/10 [==============================] - 0s 18ms/step - loss: 0.1551 - accuracy: 0.9446 - val_loss: 0.5996 - val_accuracy: 0.8312\n",
            "Epoch 54/100\n",
            "10/10 [==============================] - 0s 16ms/step - loss: 0.1358 - accuracy: 0.9577 - val_loss: 0.6588 - val_accuracy: 0.7922\n",
            "Epoch 55/100\n",
            "10/10 [==============================] - 0s 18ms/step - loss: 0.1585 - accuracy: 0.9446 - val_loss: 0.6700 - val_accuracy: 0.7922\n",
            "Epoch 56/100\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.1190 - accuracy: 0.9642 - val_loss: 0.5393 - val_accuracy: 0.8182\n",
            "Epoch 57/100\n",
            "10/10 [==============================] - 0s 18ms/step - loss: 0.1683 - accuracy: 0.9544 - val_loss: 0.5765 - val_accuracy: 0.7922\n",
            "Epoch 58/100\n",
            "10/10 [==============================] - 0s 18ms/step - loss: 0.1311 - accuracy: 0.9544 - val_loss: 0.5487 - val_accuracy: 0.8312\n",
            "Epoch 59/100\n",
            "10/10 [==============================] - 0s 18ms/step - loss: 0.1246 - accuracy: 0.9674 - val_loss: 0.4979 - val_accuracy: 0.8182\n",
            "Epoch 60/100\n",
            "10/10 [==============================] - 0s 16ms/step - loss: 0.1363 - accuracy: 0.9577 - val_loss: 0.5041 - val_accuracy: 0.7792\n",
            "Epoch 61/100\n",
            "10/10 [==============================] - 0s 16ms/step - loss: 0.1049 - accuracy: 0.9642 - val_loss: 0.6037 - val_accuracy: 0.8052\n",
            "Epoch 62/100\n",
            "10/10 [==============================] - 0s 17ms/step - loss: 0.1146 - accuracy: 0.9609 - val_loss: 0.6989 - val_accuracy: 0.8182\n",
            "Epoch 63/100\n",
            "10/10 [==============================] - 0s 17ms/step - loss: 0.1258 - accuracy: 0.9414 - val_loss: 0.6537 - val_accuracy: 0.7662\n",
            "Epoch 64/100\n",
            "10/10 [==============================] - 0s 18ms/step - loss: 0.1084 - accuracy: 0.9577 - val_loss: 0.6073 - val_accuracy: 0.8052\n",
            "Epoch 65/100\n",
            "10/10 [==============================] - 0s 19ms/step - loss: 0.1168 - accuracy: 0.9674 - val_loss: 0.7171 - val_accuracy: 0.7403\n",
            "Epoch 66/100\n",
            "10/10 [==============================] - 0s 17ms/step - loss: 0.1624 - accuracy: 0.9479 - val_loss: 0.8337 - val_accuracy: 0.7532\n",
            "Epoch 67/100\n",
            "10/10 [==============================] - 0s 16ms/step - loss: 0.1475 - accuracy: 0.9609 - val_loss: 0.8118 - val_accuracy: 0.7013\n",
            "Epoch 68/100\n",
            "10/10 [==============================] - 0s 18ms/step - loss: 0.1032 - accuracy: 0.9609 - val_loss: 0.9897 - val_accuracy: 0.6753\n",
            "Epoch 69/100\n",
            "10/10 [==============================] - 0s 17ms/step - loss: 0.1122 - accuracy: 0.9609 - val_loss: 0.8019 - val_accuracy: 0.6883\n",
            "Epoch 70/100\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.1050 - accuracy: 0.9674 - val_loss: 0.8810 - val_accuracy: 0.7143\n",
            "Epoch 71/100\n",
            "10/10 [==============================] - 0s 17ms/step - loss: 0.1053 - accuracy: 0.9642 - val_loss: 0.8765 - val_accuracy: 0.7273\n",
            "Epoch 72/100\n",
            "10/10 [==============================] - 0s 17ms/step - loss: 0.1340 - accuracy: 0.9642 - val_loss: 0.8957 - val_accuracy: 0.7662\n",
            "Epoch 73/100\n",
            "10/10 [==============================] - 0s 17ms/step - loss: 0.1047 - accuracy: 0.9707 - val_loss: 0.9345 - val_accuracy: 0.7403\n",
            "Epoch 74/100\n",
            "10/10 [==============================] - 0s 18ms/step - loss: 0.1381 - accuracy: 0.9577 - val_loss: 0.7512 - val_accuracy: 0.7403\n",
            "Epoch 75/100\n",
            "10/10 [==============================] - 0s 17ms/step - loss: 0.1487 - accuracy: 0.9446 - val_loss: 0.7492 - val_accuracy: 0.7662\n",
            "Epoch 76/100\n",
            "10/10 [==============================] - 0s 18ms/step - loss: 0.1592 - accuracy: 0.9511 - val_loss: 0.6872 - val_accuracy: 0.7403\n",
            "Epoch 77/100\n",
            "10/10 [==============================] - 0s 17ms/step - loss: 0.1331 - accuracy: 0.9577 - val_loss: 0.8930 - val_accuracy: 0.7532\n",
            "Epoch 78/100\n",
            "10/10 [==============================] - 0s 16ms/step - loss: 0.1328 - accuracy: 0.9609 - val_loss: 0.7751 - val_accuracy: 0.7792\n",
            "Epoch 79/100\n",
            "10/10 [==============================] - 0s 16ms/step - loss: 0.1152 - accuracy: 0.9642 - val_loss: 0.7939 - val_accuracy: 0.7532\n",
            "Epoch 80/100\n",
            "10/10 [==============================] - 0s 28ms/step - loss: 0.0987 - accuracy: 0.9674 - val_loss: 0.7860 - val_accuracy: 0.7013\n",
            "Epoch 81/100\n",
            "10/10 [==============================] - 0s 27ms/step - loss: 0.1063 - accuracy: 0.9674 - val_loss: 0.7241 - val_accuracy: 0.7922\n",
            "Epoch 82/100\n",
            "10/10 [==============================] - 0s 22ms/step - loss: 0.0912 - accuracy: 0.9674 - val_loss: 0.7032 - val_accuracy: 0.7532\n",
            "Epoch 83/100\n",
            "10/10 [==============================] - 0s 25ms/step - loss: 0.0972 - accuracy: 0.9642 - val_loss: 0.7912 - val_accuracy: 0.7403\n",
            "Epoch 84/100\n",
            "10/10 [==============================] - 0s 29ms/step - loss: 0.0877 - accuracy: 0.9772 - val_loss: 0.7407 - val_accuracy: 0.7403\n",
            "Epoch 85/100\n",
            "10/10 [==============================] - 0s 26ms/step - loss: 0.1238 - accuracy: 0.9739 - val_loss: 0.7571 - val_accuracy: 0.7273\n",
            "Epoch 86/100\n",
            "10/10 [==============================] - 0s 30ms/step - loss: 0.1175 - accuracy: 0.9609 - val_loss: 0.7079 - val_accuracy: 0.7532\n",
            "Epoch 87/100\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.0974 - accuracy: 0.9577 - val_loss: 0.7242 - val_accuracy: 0.7792\n",
            "Epoch 88/100\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.1003 - accuracy: 0.9837 - val_loss: 0.8365 - val_accuracy: 0.7273\n",
            "Epoch 89/100\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.1658 - accuracy: 0.9511 - val_loss: 0.8394 - val_accuracy: 0.7143\n",
            "Epoch 90/100\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.1378 - accuracy: 0.9577 - val_loss: 0.8476 - val_accuracy: 0.7532\n",
            "Epoch 91/100\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.1404 - accuracy: 0.9577 - val_loss: 1.1587 - val_accuracy: 0.6364\n",
            "Epoch 92/100\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.1125 - accuracy: 0.9642 - val_loss: 1.1537 - val_accuracy: 0.6234\n",
            "Epoch 93/100\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.1442 - accuracy: 0.9544 - val_loss: 0.8734 - val_accuracy: 0.7273\n",
            "Epoch 94/100\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.1306 - accuracy: 0.9577 - val_loss: 0.8039 - val_accuracy: 0.7403\n",
            "Epoch 95/100\n",
            "10/10 [==============================] - 0s 23ms/step - loss: 0.1144 - accuracy: 0.9544 - val_loss: 0.8669 - val_accuracy: 0.7532\n",
            "Epoch 96/100\n",
            "10/10 [==============================] - 0s 30ms/step - loss: 0.0733 - accuracy: 0.9837 - val_loss: 0.9901 - val_accuracy: 0.6883\n",
            "Epoch 97/100\n",
            "10/10 [==============================] - 0s 26ms/step - loss: 0.0901 - accuracy: 0.9609 - val_loss: 1.1072 - val_accuracy: 0.6364\n",
            "Epoch 98/100\n",
            "10/10 [==============================] - 0s 24ms/step - loss: 0.0624 - accuracy: 0.9837 - val_loss: 1.0376 - val_accuracy: 0.6883\n",
            "Epoch 99/100\n",
            "10/10 [==============================] - 0s 16ms/step - loss: 0.1515 - accuracy: 0.9609 - val_loss: 0.8207 - val_accuracy: 0.6883\n",
            "Epoch 100/100\n",
            "10/10 [==============================] - 0s 17ms/step - loss: 0.1299 - accuracy: 0.9479 - val_loss: 0.8013 - val_accuracy: 0.7013\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7da6b137cac0>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model to file\n",
        "model_2.save('model_2.h5')\n",
        "# save the mapping\n",
        "dump(mapping, open('mapping.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "yooQwX_EBZ8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vv4Q4Rkf7kn4"
      },
      "source": [
        "# Generating Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UCWctow7kn-"
      },
      "source": [
        "from pickle import load\n",
        "import numpy as np\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# generate a sequence of characters with a language model\n",
        "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
        "    in_text = seed_text\n",
        "    # generate a fixed number of characters\n",
        "    for _ in range(n_chars):\n",
        "        # encode the characters as integers\n",
        "        encoded = [mapping[char] for char in in_text]\n",
        "        # truncate sequences to a fixed length\n",
        "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "        # one hot encode\n",
        "        encoded = to_categorical(encoded, num_classes=len(mapping))\n",
        "        # predict character\n",
        "        yhat = np.argmax(model.predict(encoded), axis=-1)\n",
        "        # reverse map integer to character\n",
        "        out_char = ''\n",
        "        for char, index in mapping.items():\n",
        "            if index == yhat:\n",
        "                out_char = char\n",
        "                break\n",
        "        # append to input\n",
        "        in_text += char\n",
        "    return in_text\n",
        "\n",
        "# load the model\n",
        "model = load_model('model.h5')\n",
        "model_2 = load_model('model_2.h5')\n",
        "# load the mapping\n",
        "mapping = load(open('mapping.pkl', 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXn18EY57koA"
      },
      "source": [
        "Running the example generates three sequences of text.\n",
        "\n",
        "The first is a test to see how the model does at starting from the beginning of the rhyme. The second is a test to see how well it does at beginning in the middle of a line. The final example is a test to see how well it does with a sequence of characters never seen before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UhgrJYh7koB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e945a046-c521-4051-9b7f-9f0ee2c9adaa"
      },
      "source": [
        "# test start of rhyme\n",
        "print(generate_seq(model, mapping, 10, 'Sing a son', 20))\n",
        "# test mid-line\n",
        "print(generate_seq(model, mapping, 10, 'king was i', 20))\n",
        "# test not in original\n",
        "print(generate_seq(model, mapping, 10, 'hello worl', 20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 462ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Sing a song of sixpence,A pock\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "king was in his counting house\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "hello worleWheeda aacinn t a c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test start of rhyme\n",
        "print(generate_seq(model_2, mapping, 10, 'Sing a son', 20))\n",
        "# test mid-line\n",
        "print(generate_seq(model_2, mapping, 10, 'king was i', 20))\n",
        "# test not in original\n",
        "print(generate_seq(model_2, mapping, 10, 'hello worl', 20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W17_oajSBtWb",
        "outputId": "9ae51020-686d-4b18-e5ac-b538edb9e843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 486ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Sing a song of sixpence,A pock\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "king was in his counting house\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "hello worly.When than biras an\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## \"Stopping by Woods on a Snowy Evening\" (Robert Frost)"
      ],
      "metadata": {
        "id": "-_sOjupE1ZqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "raw = 'Whose woods these are I think I know.\\\n",
        "His house is in the village though;\\\n",
        "He will not see me stopping here\\\n",
        "To watch his woods fill up with snow.\\\n",
        "My little horse must think it queer\\\n",
        "To stop without a farmhouse near\\\n",
        "Between the woods and frozen lake\\\n",
        "The darkest evening of the year.\\\n",
        "He gives his harness bells a shake\\\n",
        "To ask if there is some mistake.\\\n",
        "The only other soundâ€™s the sweep\\\n",
        "Of easy wind and downy flake.\\\n",
        "The woods are lovely, dark and deep,\\\n",
        "But I have promises to keep,\\\n",
        "And miles to go before I sleep,\\\n",
        "And miles to go before I sleep.'\n",
        "\n",
        "# Preprocess the text\n",
        "lines = raw.split('\\n')\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)\n",
        "max_sequence_length = max([len(seq) for seq in sequences])\n",
        "\n",
        "# Vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print('Vocabulary Size:', vocab_size)\n",
        "\n",
        "# Separate into input and output\n",
        "sequences = np.array(sequences)\n",
        "X, y = sequences[:, :-1], sequences[:, -1]\n",
        "X = pad_sequences([X], maxlen=X.shape[1], padding='pre')\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "\n",
        "# Define the model\n",
        "ec_model = Sequential()\n",
        "ec_model.add(Embedding(input_dim=vocab_size, output_dim=40, input_length=max_sequence_length-1))\n",
        "ec_model.add(Dropout(0.2))\n",
        "ec_model.add(LSTM(100))\n",
        "ec_model.add(Dropout(0.2))\n",
        "ec_model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "ec_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "print(ec_model.summary())\n",
        "\n",
        "#history=complex_model.fit(X, y, epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQIh_QZd-o7S",
        "outputId": "6cc48c0f-cd6a-4523-8ecf-f4413a051e07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 74\n",
            "Model: \"sequential_30\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_28 (Embedding)    (None, 101, 40)           2960      \n",
            "                                                                 \n",
            " dropout_17 (Dropout)        (None, 101, 40)           0         \n",
            "                                                                 \n",
            " lstm_37 (LSTM)              (None, 100)               56400     \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 100)               0         \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 74)                7474      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 66834 (261.07 KB)\n",
            "Trainable params: 66834 (261.07 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, tokenizer, max_sequence_length, seed_text, next_words=2):\n",
        "    for _ in range(next_words):\n",
        "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        token_list = pad_sequences([token_list], maxlen=max_sequence_length-1, padding='pre')\n",
        "        predicted_word_index = np.argmax(model.predict(token_list), axis=-1)\n",
        "        predicted_word = tokenizer.index_word[predicted_word_index[0]]\n",
        "        seed_text += \" \" + predicted_word\n",
        "    return seed_text"
      ],
      "metadata": {
        "id": "IbOcZiuX-vXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test start of sentence\n",
        "print(generate_text(ec_model, tokenizer, max_sequence_length, 'Whose woods'))\n",
        "\n",
        "# Test mid-line\n",
        "print(generate_text(ec_model, tokenizer, max_sequence_length, 'The darkest'))\n",
        "\n",
        "# Test not in the orgiinal\n",
        "print(generate_text(ec_model, tokenizer, max_sequence_length, 'hello world'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLy3OU7J-_yh",
        "outputId": "9aa8b73c-7676-4b5c-98b6-d4f6e01daff8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 495ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Whose woods see not\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "The darkest evening must\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "hello world evening see\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I decided to run a word-level model on the well-known poem by Robert Frost, \"Stopping by Woods on a Snowy Evening.\" I chose this text because of its usage of older language and its focus on artistic expression rather than conveying straightforward information or forming logical connections between words. This made me anticipate that the model might face difficulties in handling this particular piece of text, making it a bit more challenging.\n",
        "\n",
        "While developing the model, I made a few choices to enhance its performance. I added extra layers, including an embedding layer, to consolidate the higher-dimensional 'word vectors' and refine the model's understanding of the language nuances. Keeping LSTM and Dense layers as additional layers introduced complexity, and including dropout layers helped prevent overfitting, ultimately improving the fluency of the generated sequences.\n",
        "\n",
        "Overall, my belief is that word-level models have the potential to better capture the semantic context, but only when coupled with measures to prevent overfitting and an appropriate level of complexity, as I implemented in this case. While the output was not accurate to the actual poem itself, it did seem to make some sense. The goal was to strike a balance, allowing the model to discern the subtleties of the poetic language while accounting for its inherent complexities."
      ],
      "metadata": {
        "id": "vfZ_Cqb2C53U"
      }
    }
  ]
}